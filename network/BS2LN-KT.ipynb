{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e5b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0343de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "2.5.0\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "1.1.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import function_training\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras_tuner as kt\n",
    "print(kt.__version__)\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from dataLoader import AnyDataset\n",
    "import function_training\n",
    "from config import args\n",
    "# from config import MLP_model,hyperparameters, args, CNN_model\n",
    "\n",
    "numBands=100\n",
    "bands_type=\"spin up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec220c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = {\n",
    "# #         \"learning_rate\": 0.00005,\n",
    "# #         \"epochs\": 100,\n",
    "#         \"batch_size\": 32,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffc319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "# step 0. Check GPUs available:\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "# set device\n",
    "device = tf.device('/GPU:0') if len(gpus) != 0 else tf.device('/CPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca9c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train set:  26%|█████▍               | 232/900 [00:00<00:01, 436.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2m/rcx35vxs2fd1pw444qsbggp40000gn/T/ipykernel_37546/4112608903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#take data and set batch size here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list/actual/train_set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson2inputlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"load\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numClasses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbands_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list/actual/test_set.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson2inputlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"load\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numClasses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbands_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# train_loader = tf.data.Dataset.from_tensor_slices((train_dataset.data_inputs, train_dataset.data_labels)).shuffle(train_dataset.len).batch(hyperparameters['batch_size'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/LayersTraining/BS2layergroup/network/dataLoader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_list_path, json2inputlabel, numClasses, bands_type, training)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mdata_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdata_input_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_label_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson2inputlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbands_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# step 1. prepare dataset\n",
    "def json2inputlabel(data_json, bands_type=\"spin up\"):\n",
    "    _bands_type = {\"spin up\": \"spin_up_bands\",\n",
    "                   \"spin down\": \"spin_down_bands\",\n",
    "                   \"soc\": \"soc_bands\"}\n",
    "    data_input_np = np.array(data_json[_bands_type[bands_type]]) # 100 x 400\n",
    "    # data_input_np = np.array(data_json[_bands_type[bands_type]]).flataten().T # 40000x1\n",
    "    data_label_np = np.array([data_json[\"new_label\"]])\n",
    "    # data_label_np = np.array([data_json[\"layers_num\"]])\n",
    "\n",
    "    return data_input_np, data_label_np\n",
    "\n",
    "\n",
    "#take data and set batch size here\n",
    "train_dataset = AnyDataset(\"list/actual/train_set.txt\", json2inputlabel, args[\"load\"][\"numClasses\"],bands_type, training=True)\n",
    "test_dataset = AnyDataset(\"list/actual/test_set.txt\", json2inputlabel, args[\"load\"][\"numClasses\"],bands_type, training=False)\n",
    "# train_loader = tf.data.Dataset.from_tensor_slices((train_dataset.data_inputs, train_dataset.data_labels)).shuffle(train_dataset.len).batch(hyperparameters['batch_size'])\n",
    "# test_loader = tf.data.Dataset.from_tensor_slices((test_dataset.data_inputs,test_dataset.data_labels)).shuffle(test_dataset.len).batch(hyperparameters['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(60, 100)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "#     for i in range(hp.Int('layers', 1, 5)):\n",
    "#         hppp = hp.Int(f'hidden_layer_{i}_units', 50, 100, step=10)\n",
    "#         hp_activation = hp.Choice('act_' + str(i), ['relu', 'sigmoid'])\n",
    "#         model.add(tf.keras.layers.Dense(units=hppp, activation=hp_activation))\n",
    "  \n",
    "    hp_L1 = 0.7\n",
    "#     L1_list = np.arange(0.1, 1, 0.1).tolist()\n",
    "#     hp_L1 = hp.Choice('L1 regularizer', values=L1_list)\n",
    "    \n",
    "    #TODO layer size tuning\n",
    "    hp_layers = hp.Int('layers', 3, 5)\n",
    "#     hp_layers = hp.Choice(layers, 2)\n",
    "    for i in range(hp_layers):\n",
    "#         hp_units = hp.Choice(f'hidden_layer_{i}_units', [5])\n",
    "        hp_units = hp.Choice(f'hidden_layer_{i}_units', [5, 125, 625, 3125])\n",
    "        model.add(tf.keras.layers.Dense(hp_units, use_bias=True, bias_initializer='zeros',\n",
    "                          kernel_regularizer=tf.keras.regularizers.L1(hp_L1),\n",
    "                          activity_regularizer=tf.keras.regularizers.L2(1-hp_L1)))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    ## output layers\n",
    "    model.add(tf.keras.layers.Dense(5))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    # TO CHECK\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-5, 1e-6])\n",
    "#     hp_decay_steps = hp.Choice('decay_steps', values=[1e3, 1e4])\n",
    "#     hp_decay_rate = hp.Choice('decay_rate', values=[0.8, 0.85, 0.9, 0.95])\n",
    "\n",
    "    #Small size testing \n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-5])\n",
    "#     hp_decay_steps = hp.Choice('decay_steps', values=[1e4])\n",
    "#     hp_decay_rate = hp.Choice('decay_rate', values=[0.9])\n",
    "    \n",
    "#     lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate=hp_learning_rate,\n",
    "#         decay_steps=hp_decay_steps,\n",
    "#         decay_rate=hp_decay_rate)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "    \n",
    "    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hyperparameters['learning_rate'],\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "#     with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#         hp.hparams_config(hparams=[hp_layers])\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=loss_fn,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# run parameter\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# hist_callback = tf.keras.callbacks.TensorBoard(\n",
    "#     log_dir=log_dir,\n",
    "#     histogram_freq=1,\n",
    "#     embeddings_freq=1,\n",
    "#     write_graph=True,\n",
    "# #     update_freq='batch'\n",
    "#     update_freq='epoch',\n",
    "# )\n",
    "\n",
    "hist_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    \n",
    "    histogram_freq=0,\n",
    "    update_freq=\"epoch\", #epoch or batch?\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    profile_batch=0,   \n",
    ")\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy', ###\n",
    "                     max_epochs=500,\n",
    "                     factor=3,\n",
    "                     directory=log_dir,\n",
    "#                      directory='./hp_output/',\n",
    "                     project_name='BS2LN', overwrite=True)\n",
    "\n",
    "\n",
    "print(\"log_dir\", log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data formatting\n",
    "train_dataset_input = np.array(train_dataset.data_inputs)\n",
    "test_dataset_input = np.array(test_dataset.data_inputs)\n",
    "train_dataset_labels = np.array(train_dataset.data_labels).reshape(900,)\n",
    "test_dataset_labels = np.array(test_dataset.data_labels).reshape(100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c7cb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tuner.search(train_dataset.data_inputs, test_dataset.data_inputs, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# tuner.search(train_dataset_input, train_dataset_labels, epochs=5, validation_split=0.2, callbacks=[stop_early])\n",
    "# tuner.search(train_dataset_input, train_dataset_labels, epochs=300, validation_split=0.2, callbacks=[stop_early])\n",
    "tuner.search(train_dataset_input, train_dataset_labels, epochs=400, validation_data=(test_dataset_input, test_dataset_labels), callbacks=[hist_callback], use_multiprocessing=True, verbose=1)\n",
    "# tuner.search(train_dataset_input, train_dataset_labels, epochs=100, validation_data=(test_dataset_input, test_dataset_labels), callbacks=[stop_early], use_multiprocessing=True)\n",
    "# best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# # Get the optimal hyperparameters\n",
    "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54558e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tuner.get_best_models()[0]\n",
    "#tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# print(tuner.get_best_hyperparameters())\n",
    "tuner.results_summary(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb65de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MLP_model = [\n",
    "# #     tf.keras.layers.Flatten(input_shape=(60, 100)),\n",
    "# #     # tf.keras.layers.Flatten(input_shape=(60, 400)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "# #     tf.keras.layers.Dense(10000, use_bias=True, bias_initializer='zeros',\n",
    "# #                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "# #                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "# #     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "# #     tf.keras.layers.Dense(3125, use_bias=True, bias_initializer='zeros',\n",
    "# #                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "# #                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "# #     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "\n",
    "# #     tf.keras.layers.Dense(625, use_bias=True, bias_initializer='zeros',\n",
    "# #                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "# #                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "# #     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "# #     tf.keras.layers.Dense(125, use_bias=True, bias_initializer='zeros',\n",
    "# #                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "# #                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "\n",
    "# #     tf.keras.layers.Dense(5),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "# # #     tf.keras.layers.Softmax()\n",
    "# # ]\n",
    "# MLP_model = [\n",
    "#     tf.keras.layers.Flatten(input_shape=(60, 100)),\n",
    "#     # tf.keras.layers.Flatten(input_shape=(60, 400)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "# #     tf.keras.layers.Dense(10000, use_bias=True, bias_initializer='zeros',\n",
    "# #                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "# #                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "# #     tf.keras.layers.LeakyReLU(),\n",
    "# #     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "#     tf.keras.layers.Dense(3125, use_bias=True, bias_initializer='zeros',\n",
    "#                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "#                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "\n",
    "#     tf.keras.layers.Dense(625, use_bias=True, bias_initializer='zeros',\n",
    "#                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "#                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "#     tf.keras.layers.Dense(125, use_bias=True, bias_initializer='zeros',\n",
    "#                           kernel_regularizer=tf.keras.regularizers.L1(0.7),\n",
    "#                           activity_regularizer=tf.keras.regularizers.L2(0.3)),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "\n",
    "#     tf.keras.layers.Dense(5),\n",
    "#     tf.keras.layers.LeakyReLU(),\n",
    "# #     tf.keras.layers.Softmax()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28dfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = np.array(train_dataset.data_inputs)\n",
    "# np.unique(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=810\n",
    "# array[x:x+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2. build model\n",
    "# model = tf.keras.Sequential(MLP_model)\n",
    "\n",
    "# # step 3. define loss\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # # step 4. Model compile\n",
    "# lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=hyperparameters['learning_rate'],\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.9)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "# # step 5. select metrics\n",
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88415972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 6. train & evaluate & save model\n",
    "# losses_train, accuracy_train, losses_test, accuracy_test, model = function_training.train_it(device, model, \"./state_dicts/\",\n",
    "#                                                                                       hyperparameters['epochs'],train_loader, test_loader, loss_fn, optimizer,\n",
    "#                                                                                       train_loss, train_accuracy, test_loss, test_accuracy,\n",
    "#                                                                                       eval_num_epochs=1, numClasss=args[\"load\"][\"numClasses\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648378b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_x, test_labels in test_loader:\n",
    "#     print (f\"model {model(test_x,training=False)}, actual {test_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf577a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(cm, class_names):\n",
    "#     \"\"\"\n",
    "#     Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "#     Args:\n",
    "#        cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "#        class_names (array, shape = [n]): String names of the integer classes\n",
    "#     \"\"\"\n",
    "#     plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "#     plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "    \n",
    "#     figure = plt.figure(figsize=(8, 8), dpi=400)\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#     plt.title(\"Confusion matrix\")\n",
    "# #     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(class_names))\n",
    "#     plt.xticks(tick_marks, class_names)\n",
    "# #     plt.xticks(tick_marks, class_names, rotation=45)\n",
    "#     plt.yticks(tick_marks, class_names)\n",
    "\n",
    "\n",
    "#     # Normalize the confusion matrix.\n",
    "#     cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "#     # Use white text if squares are dark; otherwise black.\n",
    "#     threshold = cm.max() / 2.\n",
    "    \n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "#             plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Prediction')\n",
    "# #     return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04605210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_loader2 = tf.data.Dataset.from_tensor_slices((test_dataset.data_inputs,test_dataset.data_labels)).shuffle(train_dataset.len).batch(1000)\n",
    "\n",
    "# # model2 = tf.keras.Sequential(MLP_model)\n",
    "\n",
    "# for element, labels in test_loader2:\n",
    "# #     print (element)\n",
    "# #     print (labels)\n",
    "# #     np.maximum([model(element, training=False)])\n",
    "#     predict = np.argmax(model.predict(element), axis=-1)\n",
    "#     label = np.array(labels).flatten()\n",
    "# #     print (f\"model {model.predict_classes(element)}, actual {labels}\")\n",
    "# #     model.predict_classes(element)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7388d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = np.array(tf.math.confusion_matrix(labels=label, predictions=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a868796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(cm, [3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (img_train, label_train), (img_test, label_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69612bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
